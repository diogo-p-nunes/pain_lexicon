{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hxMMj1SFtrC"
   },
   "source": [
    "SCRIPT TO TRAIN W2V/TFIDF ON TOKENIZED TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "R58_47wSFtrD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p22c8V7zHc8K"
   },
   "outputs": [],
   "source": [
    "# spacy\n",
    "if os.getcwd() == '/content': # on google drive\n",
    "    import spacy\n",
    "    nlp = spacy.load('en')\n",
    "else: # on CRIS azure\n",
    "    import scispacy\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_sci_md')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.examples import sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thinc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy && python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('en')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "#nlp = spacy.load('en',parse=True,tag=True, entity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSBDLTOOG1DV"
   },
   "source": [
    "## PREPARE TEXT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GkjHrx87HNTd"
   },
   "outputs": [],
   "source": [
    "def tokenize_spacy(df, text_col=None, tokenization_type='clean', outfile=None):\n",
    "    tok_snts = []\n",
    "    if outfile is not None: f = open(outfile, 'w', encoding='utf8')\n",
    "    data = df if text_col is None else df[text_col]\n",
    "    for snt in data:\n",
    "        tkns = nlp.tokenizer(snt)\n",
    "        if ('low' in tokenization_type) and ('wos' in tokenization_type):\n",
    "            _tkns = [str(x.text).lower() for x in tkns if not x.is_space]\n",
    "        elif 'wos' in tokenization_type:\n",
    "            _tkns = [str(x.text) for x in tkns if not x.is_space]\n",
    "        elif 'lem' in tokenization_type:\n",
    "            _tkns = [str(x.lemma_).lower() for x in tkns if not x.is_space and not x.is_punct]\n",
    "        elif 'stem' in tokenization_type:\n",
    "            _tkns = [stemmer.stem(str(x.text).lower()) for x in tkns if not x.is_space and not x.is_punct]\n",
    "        else: # clean by default\n",
    "            _tkns = [str(x.text).lower() for x in tkns if not x.is_space and not x.is_punct]\n",
    "        \n",
    "        if outfile is not None: # flush to file if option selected\n",
    "            f.write(\"{}\\n\".format(\"\\t\".join(_tkns)))\n",
    "        else: # otherwise save in variable\n",
    "            tok_snts.append(_tkns)\n",
    "\n",
    "    return tok_snts if outfile is None else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BKz2z9i_FtrI"
   },
   "outputs": [],
   "source": [
    "# PREPARE THE TXT FILE\n",
    "raw_file = './NOTEEVENTS.csv'\n",
    "df = pd.read_csv(raw_file)\n",
    "sentences = tokenize_spacy(df, text_col=None, tokenization_type='clean', outfile=raw_file)\n",
    "#sentences = tokenize_spacy(df, text_col=None, tokenization_type='clean', outfile=raw_file, replace('.csv','_clean.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('NOTEEVENTS_final_clean.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lower_no_punc_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'76' 'yo' 'm' 'initially' 'admitted' 'to' '' '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'54yr' 'man' 'with' 'hx' 'metastatic' 'renal' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'chief' 'complaint' '' '24' 'hour' 'events' ''...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'chief' 'complaint' '' 'acute' 'hepatitis' 'hp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'chief' 'complaint' '' 'hpi' '' '24' 'hour' 'e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                lower_no_punc_tokens\n",
       "0  '76' 'yo' 'm' 'initially' 'admitted' 'to' '' '...\n",
       "1  '54yr' 'man' 'with' 'hx' 'metastatic' 'renal' ...\n",
       "2  'chief' 'complaint' '' '24' 'hour' 'events' ''...\n",
       "3  'chief' 'complaint' '' 'acute' 'hepatitis' 'hp...\n",
       "4  'chief' 'complaint' '' 'hpi' '' '24' 'hour' 'e..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_array = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"noteevents_clean.txt\", numpy_array, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN9f0bADHJ5Z"
   },
   "source": [
    "## INPUT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mfhDR7-NFtrK",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# INPUT PARAMETERS\n",
    "\n",
    "# tokenized text\n",
    "#filename = '/home/ubuntu/data/mimic/tokenized_scispacy/noteevents_tkns_stem.txt' # with stemming\n",
    "filename = './noteevents_clean.txt'\n",
    "\n",
    "# w2v/fasttext params\n",
    "size=300\n",
    "window=10\n",
    "min_count=5\n",
    "workers=16\n",
    "\n",
    "# sklearn - tfidf params\n",
    "min_df=0.2\n",
    "max_df=0.9\n",
    "ngram_range=(1, 5)\n",
    "max_features=1000\n",
    "\n",
    "# gensim - tfidf params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qxpI2r6AFtrN"
   },
   "outputs": [],
   "source": [
    "class SentenceIterator: \n",
    "    def __init__(self, filepath): \n",
    "        self.filepath = filepath \n",
    "\n",
    "    def __iter__(self): \n",
    "        for line in open(self.filepath): \n",
    "            yield line.split(\"\\t\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "tVQesBXYFtrP"
   },
   "outputs": [],
   "source": [
    "sentences = SentenceIterator(filename) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lc7l9N7mG4zu"
   },
   "source": [
    "## TRAIN GLOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OxDvczFFtrR"
   },
   "outputs": [],
   "source": [
    "# TRAIN AND SAVE GLOVE\n",
    "import glove\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "\n",
    "corpus = Corpus() #Creating a corpus object\n",
    "corpus.fit(sentences, window=window) #Training the corpus to generate the co occurence matrix which is used in GloVe\n",
    "\n",
    "glove = Glove(no_components=5, learning_rate=0.05) \n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save(filename.replace('.txt','_glove.model').replace('_tkns',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlEdVoQVG74_"
   },
   "source": [
    "## TRAIN FASTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "adVl5M5pFtrU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 10:58:57,837 : INFO : resetting layer weights\n",
      "2021-02-22 10:59:13,126 : INFO : collecting all words and their counts\n",
      "2021-02-22 10:59:13,128 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-02-22 10:59:13,262 : INFO : PROGRESS: at sentence #10000, processed 10000 words, keeping 9349 word types\n",
      "2021-02-22 10:59:13,398 : INFO : PROGRESS: at sentence #20000, processed 20000 words, keeping 18556 word types\n",
      "2021-02-22 10:59:13,529 : INFO : PROGRESS: at sentence #30000, processed 30000 words, keeping 27752 word types\n",
      "2021-02-22 10:59:13,628 : INFO : PROGRESS: at sentence #40000, processed 40000 words, keeping 37343 word types\n",
      "2021-02-22 10:59:13,695 : INFO : PROGRESS: at sentence #50000, processed 50000 words, keeping 47343 word types\n",
      "2021-02-22 10:59:13,761 : INFO : PROGRESS: at sentence #60000, processed 60000 words, keeping 57343 word types\n",
      "2021-02-22 10:59:13,827 : INFO : PROGRESS: at sentence #70000, processed 70000 words, keeping 67343 word types\n",
      "2021-02-22 10:59:13,894 : INFO : PROGRESS: at sentence #80000, processed 80000 words, keeping 77343 word types\n",
      "2021-02-22 10:59:13,962 : INFO : PROGRESS: at sentence #90000, processed 90000 words, keeping 87343 word types\n",
      "2021-02-22 10:59:14,032 : INFO : PROGRESS: at sentence #100000, processed 100000 words, keeping 97335 word types\n",
      "2021-02-22 10:59:14,100 : INFO : PROGRESS: at sentence #110000, processed 110000 words, keeping 107335 word types\n",
      "2021-02-22 10:59:14,167 : INFO : PROGRESS: at sentence #120000, processed 120000 words, keeping 117335 word types\n",
      "2021-02-22 10:59:14,234 : INFO : PROGRESS: at sentence #130000, processed 130000 words, keeping 127335 word types\n",
      "2021-02-22 10:59:14,305 : INFO : PROGRESS: at sentence #140000, processed 140000 words, keeping 137335 word types\n",
      "2021-02-22 10:59:14,375 : INFO : PROGRESS: at sentence #150000, processed 150000 words, keeping 147335 word types\n",
      "2021-02-22 10:59:14,448 : INFO : PROGRESS: at sentence #160000, processed 160000 words, keeping 157335 word types\n",
      "2021-02-22 10:59:14,522 : INFO : PROGRESS: at sentence #170000, processed 170000 words, keeping 167335 word types\n",
      "2021-02-22 10:59:14,598 : INFO : PROGRESS: at sentence #180000, processed 180000 words, keeping 177335 word types\n",
      "2021-02-22 10:59:14,665 : INFO : PROGRESS: at sentence #190000, processed 190000 words, keeping 187335 word types\n",
      "2021-02-22 10:59:14,733 : INFO : PROGRESS: at sentence #200000, processed 200000 words, keeping 197335 word types\n",
      "2021-02-22 10:59:14,804 : INFO : PROGRESS: at sentence #210000, processed 210000 words, keeping 207335 word types\n",
      "2021-02-22 10:59:14,877 : INFO : PROGRESS: at sentence #220000, processed 220000 words, keeping 217335 word types\n",
      "2021-02-22 10:59:14,951 : INFO : PROGRESS: at sentence #230000, processed 230000 words, keeping 227335 word types\n",
      "2021-02-22 10:59:15,026 : INFO : PROGRESS: at sentence #240000, processed 240000 words, keeping 237335 word types\n",
      "2021-02-22 10:59:15,104 : INFO : PROGRESS: at sentence #250000, processed 250000 words, keeping 247335 word types\n",
      "2021-02-22 10:59:15,178 : INFO : PROGRESS: at sentence #260000, processed 260000 words, keeping 257335 word types\n",
      "2021-02-22 10:59:15,252 : INFO : PROGRESS: at sentence #270000, processed 270000 words, keeping 267335 word types\n",
      "2021-02-22 10:59:15,328 : INFO : PROGRESS: at sentence #280000, processed 280000 words, keeping 277335 word types\n",
      "2021-02-22 10:59:15,402 : INFO : PROGRESS: at sentence #290000, processed 290000 words, keeping 287335 word types\n",
      "2021-02-22 10:59:15,469 : INFO : PROGRESS: at sentence #300000, processed 300000 words, keeping 297329 word types\n",
      "2021-02-22 10:59:15,529 : INFO : PROGRESS: at sentence #310000, processed 310000 words, keeping 307302 word types\n",
      "2021-02-22 10:59:15,583 : INFO : PROGRESS: at sentence #320000, processed 320000 words, keeping 317264 word types\n",
      "2021-02-22 10:59:15,642 : INFO : PROGRESS: at sentence #330000, processed 330000 words, keeping 327237 word types\n",
      "2021-02-22 10:59:15,696 : INFO : PROGRESS: at sentence #340000, processed 340000 words, keeping 337201 word types\n",
      "2021-02-22 10:59:15,752 : INFO : PROGRESS: at sentence #350000, processed 350000 words, keeping 347146 word types\n",
      "2021-02-22 10:59:15,821 : INFO : PROGRESS: at sentence #360000, processed 360000 words, keeping 357101 word types\n",
      "2021-02-22 10:59:15,874 : INFO : PROGRESS: at sentence #370000, processed 370000 words, keeping 367048 word types\n",
      "2021-02-22 10:59:15,927 : INFO : PROGRESS: at sentence #380000, processed 380000 words, keeping 377006 word types\n",
      "2021-02-22 10:59:15,977 : INFO : PROGRESS: at sentence #390000, processed 390000 words, keeping 386928 word types\n",
      "2021-02-22 10:59:16,028 : INFO : PROGRESS: at sentence #400000, processed 400000 words, keeping 396884 word types\n",
      "2021-02-22 10:59:16,081 : INFO : PROGRESS: at sentence #410000, processed 410000 words, keeping 406832 word types\n",
      "2021-02-22 10:59:16,135 : INFO : PROGRESS: at sentence #420000, processed 420000 words, keeping 416784 word types\n",
      "2021-02-22 10:59:16,190 : INFO : PROGRESS: at sentence #430000, processed 430000 words, keeping 426723 word types\n",
      "2021-02-22 10:59:16,242 : INFO : PROGRESS: at sentence #440000, processed 440000 words, keeping 436668 word types\n",
      "2021-02-22 10:59:16,299 : INFO : PROGRESS: at sentence #450000, processed 450000 words, keeping 446622 word types\n",
      "2021-02-22 10:59:16,354 : INFO : PROGRESS: at sentence #460000, processed 460000 words, keeping 456565 word types\n",
      "2021-02-22 10:59:16,406 : INFO : PROGRESS: at sentence #470000, processed 470000 words, keeping 466520 word types\n",
      "2021-02-22 10:59:16,458 : INFO : PROGRESS: at sentence #480000, processed 480000 words, keeping 476460 word types\n",
      "2021-02-22 10:59:16,510 : INFO : PROGRESS: at sentence #490000, processed 490000 words, keeping 486384 word types\n",
      "2021-02-22 10:59:16,563 : INFO : PROGRESS: at sentence #500000, processed 500000 words, keeping 496291 word types\n",
      "2021-02-22 10:59:16,608 : INFO : PROGRESS: at sentence #510000, processed 510000 words, keeping 506182 word types\n",
      "2021-02-22 10:59:16,652 : INFO : PROGRESS: at sentence #520000, processed 520000 words, keeping 516046 word types\n",
      "2021-02-22 10:59:16,697 : INFO : PROGRESS: at sentence #530000, processed 530000 words, keeping 525884 word types\n",
      "2021-02-22 10:59:16,739 : INFO : PROGRESS: at sentence #540000, processed 540000 words, keeping 535710 word types\n",
      "2021-02-22 10:59:16,781 : INFO : PROGRESS: at sentence #550000, processed 550000 words, keeping 545525 word types\n",
      "2021-02-22 10:59:16,825 : INFO : PROGRESS: at sentence #560000, processed 560000 words, keeping 555358 word types\n",
      "2021-02-22 10:59:16,868 : INFO : PROGRESS: at sentence #570000, processed 570000 words, keeping 565158 word types\n",
      "2021-02-22 10:59:16,913 : INFO : PROGRESS: at sentence #580000, processed 580000 words, keeping 574934 word types\n",
      "2021-02-22 10:59:16,955 : INFO : PROGRESS: at sentence #590000, processed 590000 words, keeping 584713 word types\n",
      "2021-02-22 10:59:16,995 : INFO : PROGRESS: at sentence #600000, processed 600000 words, keeping 594530 word types\n",
      "2021-02-22 10:59:17,033 : INFO : PROGRESS: at sentence #610000, processed 610000 words, keeping 604346 word types\n",
      "2021-02-22 10:59:17,071 : INFO : PROGRESS: at sentence #620000, processed 620000 words, keeping 614108 word types\n",
      "2021-02-22 10:59:17,110 : INFO : PROGRESS: at sentence #630000, processed 630000 words, keeping 623921 word types\n",
      "2021-02-22 10:59:17,148 : INFO : PROGRESS: at sentence #640000, processed 640000 words, keeping 633714 word types\n",
      "2021-02-22 10:59:17,186 : INFO : PROGRESS: at sentence #650000, processed 650000 words, keeping 643489 word types\n",
      "2021-02-22 10:59:17,226 : INFO : PROGRESS: at sentence #660000, processed 660000 words, keeping 653292 word types\n",
      "2021-02-22 10:59:17,264 : INFO : PROGRESS: at sentence #670000, processed 670000 words, keeping 663081 word types\n",
      "2021-02-22 10:59:17,302 : INFO : PROGRESS: at sentence #680000, processed 680000 words, keeping 672801 word types\n",
      "2021-02-22 10:59:17,339 : INFO : PROGRESS: at sentence #690000, processed 690000 words, keeping 682553 word types\n",
      "2021-02-22 10:59:17,378 : INFO : PROGRESS: at sentence #700000, processed 700000 words, keeping 692346 word types\n",
      "2021-02-22 10:59:17,456 : INFO : PROGRESS: at sentence #710000, processed 710000 words, keeping 702170 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 10:59:17,508 : INFO : PROGRESS: at sentence #720000, processed 720000 words, keeping 712016 word types\n",
      "2021-02-22 10:59:17,566 : INFO : PROGRESS: at sentence #730000, processed 730000 words, keeping 721827 word types\n",
      "2021-02-22 10:59:17,851 : INFO : PROGRESS: at sentence #740000, processed 740000 words, keeping 731787 word types\n",
      "2021-02-22 10:59:18,167 : INFO : PROGRESS: at sentence #750000, processed 750000 words, keeping 741771 word types\n",
      "2021-02-22 10:59:18,516 : INFO : PROGRESS: at sentence #760000, processed 760000 words, keeping 751761 word types\n",
      "2021-02-22 10:59:18,605 : INFO : PROGRESS: at sentence #770000, processed 770000 words, keeping 761756 word types\n",
      "2021-02-22 10:59:18,699 : INFO : PROGRESS: at sentence #780000, processed 780000 words, keeping 771742 word types\n",
      "2021-02-22 10:59:18,758 : INFO : PROGRESS: at sentence #790000, processed 790000 words, keeping 781413 word types\n",
      "2021-02-22 10:59:18,781 : INFO : PROGRESS: at sentence #800000, processed 800000 words, keeping 790809 word types\n",
      "2021-02-22 10:59:18,804 : INFO : PROGRESS: at sentence #810000, processed 810000 words, keeping 800253 word types\n",
      "2021-02-22 10:59:18,825 : INFO : PROGRESS: at sentence #820000, processed 820000 words, keeping 809679 word types\n",
      "2021-02-22 10:59:18,847 : INFO : PROGRESS: at sentence #830000, processed 830000 words, keeping 819020 word types\n",
      "2021-02-22 10:59:18,869 : INFO : PROGRESS: at sentence #840000, processed 840000 words, keeping 828299 word types\n",
      "2021-02-22 10:59:18,890 : INFO : PROGRESS: at sentence #850000, processed 850000 words, keeping 837363 word types\n",
      "2021-02-22 10:59:18,911 : INFO : PROGRESS: at sentence #860000, processed 860000 words, keeping 846463 word types\n",
      "2021-02-22 10:59:18,933 : INFO : PROGRESS: at sentence #870000, processed 870000 words, keeping 855672 word types\n",
      "2021-02-22 10:59:18,955 : INFO : PROGRESS: at sentence #880000, processed 880000 words, keeping 864956 word types\n",
      "2021-02-22 10:59:18,993 : INFO : PROGRESS: at sentence #890000, processed 890000 words, keeping 874076 word types\n",
      "2021-02-22 10:59:19,084 : INFO : PROGRESS: at sentence #900000, processed 900000 words, keeping 883306 word types\n",
      "2021-02-22 10:59:19,198 : INFO : PROGRESS: at sentence #910000, processed 910000 words, keeping 892451 word types\n",
      "2021-02-22 10:59:19,309 : INFO : PROGRESS: at sentence #920000, processed 920000 words, keeping 901573 word types\n",
      "2021-02-22 10:59:19,423 : INFO : PROGRESS: at sentence #930000, processed 930000 words, keeping 910768 word types\n",
      "2021-02-22 10:59:19,540 : INFO : PROGRESS: at sentence #940000, processed 940000 words, keeping 919993 word types\n",
      "2021-02-22 10:59:19,645 : INFO : PROGRESS: at sentence #950000, processed 950000 words, keeping 929149 word types\n",
      "2021-02-22 10:59:19,755 : INFO : PROGRESS: at sentence #960000, processed 960000 words, keeping 938368 word types\n",
      "2021-02-22 10:59:19,863 : INFO : PROGRESS: at sentence #970000, processed 970000 words, keeping 947550 word types\n",
      "2021-02-22 10:59:19,977 : INFO : PROGRESS: at sentence #980000, processed 980000 words, keeping 956811 word types\n",
      "2021-02-22 10:59:20,090 : INFO : PROGRESS: at sentence #990000, processed 990000 words, keeping 966010 word types\n",
      "2021-02-22 10:59:20,200 : INFO : PROGRESS: at sentence #1000000, processed 1000000 words, keeping 975148 word types\n",
      "2021-02-22 10:59:20,311 : INFO : PROGRESS: at sentence #1010000, processed 1010000 words, keeping 984277 word types\n",
      "2021-02-22 10:59:20,414 : INFO : PROGRESS: at sentence #1020000, processed 1020000 words, keeping 993446 word types\n",
      "2021-02-22 10:59:20,524 : INFO : PROGRESS: at sentence #1030000, processed 1030000 words, keeping 1002641 word types\n",
      "2021-02-22 10:59:20,635 : INFO : PROGRESS: at sentence #1040000, processed 1040000 words, keeping 1011914 word types\n",
      "2021-02-22 10:59:20,752 : INFO : PROGRESS: at sentence #1050000, processed 1050000 words, keeping 1021082 word types\n",
      "2021-02-22 10:59:20,864 : INFO : PROGRESS: at sentence #1060000, processed 1060000 words, keeping 1030162 word types\n",
      "2021-02-22 10:59:20,970 : INFO : PROGRESS: at sentence #1070000, processed 1070000 words, keeping 1039329 word types\n",
      "2021-02-22 10:59:21,081 : INFO : PROGRESS: at sentence #1080000, processed 1080000 words, keeping 1048532 word types\n",
      "2021-02-22 10:59:21,193 : INFO : PROGRESS: at sentence #1090000, processed 1090000 words, keeping 1057783 word types\n",
      "2021-02-22 10:59:21,304 : INFO : PROGRESS: at sentence #1100000, processed 1100000 words, keeping 1066941 word types\n",
      "2021-02-22 10:59:21,415 : INFO : PROGRESS: at sentence #1110000, processed 1110000 words, keeping 1076149 word types\n",
      "2021-02-22 10:59:21,478 : INFO : PROGRESS: at sentence #1120000, processed 1120000 words, keeping 1086104 word types\n",
      "2021-02-22 10:59:21,541 : INFO : PROGRESS: at sentence #1130000, processed 1130000 words, keeping 1096104 word types\n",
      "2021-02-22 10:59:21,608 : INFO : PROGRESS: at sentence #1140000, processed 1140000 words, keeping 1106104 word types\n",
      "2021-02-22 10:59:21,674 : INFO : PROGRESS: at sentence #1150000, processed 1150000 words, keeping 1116104 word types\n",
      "2021-02-22 10:59:21,740 : INFO : PROGRESS: at sentence #1160000, processed 1160000 words, keeping 1126104 word types\n",
      "2021-02-22 10:59:21,805 : INFO : PROGRESS: at sentence #1170000, processed 1170000 words, keeping 1136104 word types\n",
      "2021-02-22 10:59:21,870 : INFO : PROGRESS: at sentence #1180000, processed 1180000 words, keeping 1146104 word types\n",
      "2021-02-22 10:59:21,933 : INFO : PROGRESS: at sentence #1190000, processed 1190000 words, keeping 1156104 word types\n",
      "2021-02-22 10:59:22,000 : INFO : PROGRESS: at sentence #1200000, processed 1200000 words, keeping 1166104 word types\n",
      "2021-02-22 10:59:22,065 : INFO : PROGRESS: at sentence #1210000, processed 1210000 words, keeping 1176104 word types\n",
      "2021-02-22 10:59:22,133 : INFO : PROGRESS: at sentence #1220000, processed 1220000 words, keeping 1186104 word types\n",
      "2021-02-22 10:59:22,202 : INFO : PROGRESS: at sentence #1230000, processed 1230000 words, keeping 1196104 word types\n",
      "2021-02-22 10:59:22,274 : INFO : PROGRESS: at sentence #1240000, processed 1240000 words, keeping 1206104 word types\n",
      "2021-02-22 10:59:22,344 : INFO : PROGRESS: at sentence #1250000, processed 1250000 words, keeping 1216104 word types\n",
      "2021-02-22 10:59:22,415 : INFO : PROGRESS: at sentence #1260000, processed 1260000 words, keeping 1226103 word types\n",
      "2021-02-22 10:59:22,478 : INFO : PROGRESS: at sentence #1270000, processed 1270000 words, keeping 1236089 word types\n",
      "2021-02-22 10:59:22,530 : INFO : PROGRESS: at sentence #1280000, processed 1280000 words, keeping 1246042 word types\n",
      "2021-02-22 10:59:22,591 : INFO : PROGRESS: at sentence #1290000, processed 1290000 words, keeping 1256042 word types\n",
      "2021-02-22 10:59:22,655 : INFO : PROGRESS: at sentence #1300000, processed 1300000 words, keeping 1266042 word types\n",
      "2021-02-22 10:59:22,718 : INFO : PROGRESS: at sentence #1310000, processed 1310000 words, keeping 1276042 word types\n",
      "2021-02-22 10:59:22,783 : INFO : PROGRESS: at sentence #1320000, processed 1320000 words, keeping 1286042 word types\n",
      "2021-02-22 10:59:22,848 : INFO : PROGRESS: at sentence #1330000, processed 1330000 words, keeping 1296042 word types\n",
      "2021-02-22 10:59:22,912 : INFO : PROGRESS: at sentence #1340000, processed 1340000 words, keeping 1306042 word types\n",
      "2021-02-22 10:59:22,977 : INFO : PROGRESS: at sentence #1350000, processed 1350000 words, keeping 1316042 word types\n",
      "2021-02-22 10:59:23,038 : INFO : PROGRESS: at sentence #1360000, processed 1360000 words, keeping 1326042 word types\n",
      "2021-02-22 10:59:23,102 : INFO : PROGRESS: at sentence #1370000, processed 1370000 words, keeping 1336042 word types\n",
      "2021-02-22 10:59:23,160 : INFO : PROGRESS: at sentence #1380000, processed 1380000 words, keeping 1346010 word types\n",
      "2021-02-22 10:59:23,214 : INFO : PROGRESS: at sentence #1390000, processed 1390000 words, keeping 1355961 word types\n",
      "2021-02-22 10:59:23,264 : INFO : PROGRESS: at sentence #1400000, processed 1400000 words, keeping 1365893 word types\n",
      "2021-02-22 10:59:23,315 : INFO : PROGRESS: at sentence #1410000, processed 1410000 words, keeping 1375820 word types\n",
      "2021-02-22 10:59:23,367 : INFO : PROGRESS: at sentence #1420000, processed 1420000 words, keeping 1385731 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 10:59:23,417 : INFO : PROGRESS: at sentence #1430000, processed 1430000 words, keeping 1395642 word types\n",
      "2021-02-22 10:59:23,542 : INFO : PROGRESS: at sentence #1440000, processed 1440000 words, keeping 1405507 word types\n",
      "2021-02-22 10:59:23,580 : INFO : PROGRESS: at sentence #1450000, processed 1450000 words, keeping 1415287 word types\n",
      "2021-02-22 10:59:23,618 : INFO : PROGRESS: at sentence #1460000, processed 1460000 words, keeping 1425039 word types\n",
      "2021-02-22 10:59:23,658 : INFO : PROGRESS: at sentence #1470000, processed 1470000 words, keeping 1434785 word types\n",
      "2021-02-22 10:59:23,696 : INFO : PROGRESS: at sentence #1480000, processed 1480000 words, keeping 1444531 word types\n",
      "2021-02-22 10:59:23,735 : INFO : PROGRESS: at sentence #1490000, processed 1490000 words, keeping 1454304 word types\n",
      "2021-02-22 10:59:23,774 : INFO : PROGRESS: at sentence #1500000, processed 1500000 words, keeping 1464090 word types\n",
      "2021-02-22 10:59:23,812 : INFO : PROGRESS: at sentence #1510000, processed 1510000 words, keeping 1473804 word types\n",
      "2021-02-22 10:59:23,851 : INFO : PROGRESS: at sentence #1520000, processed 1520000 words, keeping 1483576 word types\n",
      "2021-02-22 10:59:23,888 : INFO : PROGRESS: at sentence #1530000, processed 1530000 words, keeping 1493369 word types\n",
      "2021-02-22 10:59:23,946 : INFO : PROGRESS: at sentence #1540000, processed 1540000 words, keeping 1503140 word types\n",
      "2021-02-22 10:59:24,254 : INFO : PROGRESS: at sentence #1550000, processed 1550000 words, keeping 1513127 word types\n",
      "2021-02-22 10:59:24,582 : INFO : PROGRESS: at sentence #1560000, processed 1560000 words, keeping 1523114 word types\n",
      "2021-02-22 10:59:24,856 : INFO : PROGRESS: at sentence #1570000, processed 1570000 words, keeping 1533099 word types\n",
      "2021-02-22 10:59:24,943 : INFO : PROGRESS: at sentence #1580000, processed 1580000 words, keeping 1543098 word types\n",
      "2021-02-22 10:59:25,035 : INFO : PROGRESS: at sentence #1590000, processed 1590000 words, keeping 1553010 word types\n",
      "2021-02-22 10:59:25,078 : INFO : PROGRESS: at sentence #1600000, processed 1600000 words, keeping 1562388 word types\n",
      "2021-02-22 10:59:25,100 : INFO : PROGRESS: at sentence #1610000, processed 1610000 words, keeping 1571600 word types\n",
      "2021-02-22 10:59:25,122 : INFO : PROGRESS: at sentence #1620000, processed 1620000 words, keeping 1580749 word types\n",
      "2021-02-22 10:59:25,143 : INFO : PROGRESS: at sentence #1630000, processed 1630000 words, keeping 1589947 word types\n",
      "2021-02-22 10:59:25,219 : INFO : PROGRESS: at sentence #1640000, processed 1640000 words, keeping 1599088 word types\n",
      "2021-02-22 10:59:25,333 : INFO : PROGRESS: at sentence #1650000, processed 1650000 words, keeping 1608230 word types\n",
      "2021-02-22 10:59:25,447 : INFO : PROGRESS: at sentence #1660000, processed 1660000 words, keeping 1617407 word types\n",
      "2021-02-22 10:59:25,565 : INFO : PROGRESS: at sentence #1670000, processed 1670000 words, keeping 1626617 word types\n",
      "2021-02-22 10:59:25,670 : INFO : PROGRESS: at sentence #1680000, processed 1680000 words, keeping 1635665 word types\n",
      "2021-02-22 10:59:25,769 : INFO : PROGRESS: at sentence #1690000, processed 1690000 words, keeping 1644827 word types\n",
      "2021-02-22 10:59:25,871 : INFO : PROGRESS: at sentence #1700000, processed 1700000 words, keeping 1654060 word types\n",
      "2021-02-22 10:59:25,978 : INFO : PROGRESS: at sentence #1710000, processed 1710000 words, keeping 1663268 word types\n",
      "2021-02-22 10:59:26,088 : INFO : PROGRESS: at sentence #1720000, processed 1720000 words, keeping 1672429 word types\n",
      "2021-02-22 10:59:26,199 : INFO : PROGRESS: at sentence #1730000, processed 1730000 words, keeping 1681571 word types\n",
      "2021-02-22 10:59:26,302 : INFO : PROGRESS: at sentence #1740000, processed 1740000 words, keeping 1690652 word types\n",
      "2021-02-22 10:59:26,410 : INFO : PROGRESS: at sentence #1750000, processed 1750000 words, keeping 1699887 word types\n",
      "2021-02-22 10:59:26,520 : INFO : PROGRESS: at sentence #1760000, processed 1760000 words, keeping 1709056 word types\n",
      "2021-02-22 10:59:26,633 : INFO : PROGRESS: at sentence #1770000, processed 1770000 words, keeping 1718223 word types\n",
      "2021-02-22 10:59:26,744 : INFO : PROGRESS: at sentence #1780000, processed 1780000 words, keeping 1727328 word types\n",
      "2021-02-22 10:59:26,849 : INFO : PROGRESS: at sentence #1790000, processed 1790000 words, keeping 1736504 word types\n",
      "2021-02-22 10:59:26,927 : INFO : PROGRESS: at sentence #1800000, processed 1800000 words, keeping 1746043 word types\n",
      "2021-02-22 10:59:26,974 : INFO : PROGRESS: at sentence #1810000, processed 1810000 words, keeping 1755986 word types\n",
      "2021-02-22 10:59:27,021 : INFO : PROGRESS: at sentence #1820000, processed 1820000 words, keeping 1765927 word types\n",
      "2021-02-22 10:59:27,070 : INFO : PROGRESS: at sentence #1830000, processed 1830000 words, keeping 1775854 word types\n",
      "2021-02-22 10:59:27,122 : INFO : PROGRESS: at sentence #1840000, processed 1840000 words, keeping 1785786 word types\n",
      "2021-02-22 10:59:27,169 : INFO : PROGRESS: at sentence #1850000, processed 1850000 words, keeping 1795728 word types\n",
      "2021-02-22 10:59:27,215 : INFO : PROGRESS: at sentence #1860000, processed 1860000 words, keeping 1805675 word types\n",
      "2021-02-22 10:59:27,263 : INFO : PROGRESS: at sentence #1870000, processed 1870000 words, keeping 1815617 word types\n",
      "2021-02-22 10:59:27,310 : INFO : PROGRESS: at sentence #1880000, processed 1880000 words, keeping 1825552 word types\n",
      "2021-02-22 10:59:27,357 : INFO : PROGRESS: at sentence #1890000, processed 1890000 words, keeping 1835478 word types\n",
      "2021-02-22 10:59:27,404 : INFO : PROGRESS: at sentence #1900000, processed 1900000 words, keeping 1845409 word types\n",
      "2021-02-22 10:59:27,452 : INFO : PROGRESS: at sentence #1910000, processed 1910000 words, keeping 1855346 word types\n",
      "2021-02-22 10:59:27,500 : INFO : PROGRESS: at sentence #1920000, processed 1920000 words, keeping 1865294 word types\n",
      "2021-02-22 10:59:27,549 : INFO : PROGRESS: at sentence #1930000, processed 1930000 words, keeping 1875221 word types\n",
      "2021-02-22 10:59:27,592 : INFO : PROGRESS: at sentence #1940000, processed 1940000 words, keeping 1885007 word types\n",
      "2021-02-22 10:59:27,633 : INFO : PROGRESS: at sentence #1950000, processed 1950000 words, keeping 1894742 word types\n",
      "2021-02-22 10:59:27,672 : INFO : PROGRESS: at sentence #1960000, processed 1960000 words, keeping 1904444 word types\n",
      "2021-02-22 10:59:27,714 : INFO : PROGRESS: at sentence #1970000, processed 1970000 words, keeping 1914147 word types\n",
      "2021-02-22 10:59:27,753 : INFO : PROGRESS: at sentence #1980000, processed 1980000 words, keeping 1923877 word types\n",
      "2021-02-22 10:59:27,795 : INFO : PROGRESS: at sentence #1990000, processed 1990000 words, keeping 1933557 word types\n",
      "2021-02-22 10:59:27,836 : INFO : PROGRESS: at sentence #2000000, processed 2000000 words, keeping 1943329 word types\n",
      "2021-02-22 10:59:27,886 : INFO : PROGRESS: at sentence #2010000, processed 2010000 words, keeping 1953058 word types\n",
      "2021-02-22 10:59:27,926 : INFO : PROGRESS: at sentence #2020000, processed 2020000 words, keeping 1962584 word types\n",
      "2021-02-22 10:59:27,947 : INFO : PROGRESS: at sentence #2030000, processed 2030000 words, keeping 1971747 word types\n",
      "2021-02-22 10:59:27,968 : INFO : PROGRESS: at sentence #2040000, processed 2040000 words, keeping 1980887 word types\n",
      "2021-02-22 10:59:27,988 : INFO : PROGRESS: at sentence #2050000, processed 2050000 words, keeping 1989929 word types\n",
      "2021-02-22 10:59:28,008 : INFO : PROGRESS: at sentence #2060000, processed 2060000 words, keeping 1998855 word types\n",
      "2021-02-22 10:59:28,030 : INFO : PROGRESS: at sentence #2070000, processed 2070000 words, keeping 2007829 word types\n",
      "2021-02-22 10:59:28,092 : INFO : PROGRESS: at sentence #2080000, processed 2080000 words, keeping 2017081 word types\n",
      "2021-02-22 10:59:28,104 : INFO : collected 2019963 word types from a corpus of 2083112 raw words and 2083112 sentences\n",
      "2021-02-22 10:59:28,104 : INFO : Loading a fresh vocabulary\n",
      "2021-02-22 10:59:28,901 : INFO : effective_min_count=5 retains 1176 unique words (0% of original 2019963, drops 2018787)\n",
      "2021-02-22 10:59:28,902 : INFO : effective_min_count=5 leaves 23371 word corpus (1% of original 2083112, drops 2059741)\n",
      "2021-02-22 10:59:28,906 : INFO : deleting the raw counts dictionary of 2019963 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 10:59:29,173 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2021-02-22 10:59:29,174 : INFO : downsampling leaves estimated 15132 word corpus (64.7% of prior 23371)\n",
      "2021-02-22 10:59:29,656 : INFO : estimated required memory for 1176 words, 78129 buckets and 300 dimensions: 110096912 bytes\n",
      "2021-02-22 10:59:29,658 : INFO : resetting layer weights\n",
      "2021-02-22 10:59:45,575 : INFO : training model with 3 workers on 1176 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-02-22 10:59:46,579 : INFO : EPOCH 1 - PROGRESS: at 4.80% examples, 304 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 10:59:47,614 : INFO : EPOCH 1 - PROGRESS: at 11.52% examples, 149 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 10:59:48,679 : INFO : EPOCH 1 - PROGRESS: at 18.72% examples, 165 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 10:59:49,752 : INFO : EPOCH 1 - PROGRESS: at 27.84% examples, 333 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 10:59:50,759 : INFO : EPOCH 1 - PROGRESS: at 35.52% examples, 487 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 10:59:51,950 : INFO : EPOCH 1 - PROGRESS: at 37.92% examples, 424 words/s, in_qsize 4, out_qsize 0\n",
      "2021-02-22 10:59:53,045 : INFO : EPOCH 1 - PROGRESS: at 46.56% examples, 914 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 10:59:54,095 : INFO : EPOCH 1 - PROGRESS: at 50.89% examples, 899 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 10:59:55,124 : INFO : EPOCH 1 - PROGRESS: at 56.17% examples, 843 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 10:59:56,124 : INFO : EPOCH 1 - PROGRESS: at 62.89% examples, 765 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 10:59:57,149 : INFO : EPOCH 1 - PROGRESS: at 70.09% examples, 734 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 10:59:58,164 : INFO : EPOCH 1 - PROGRESS: at 74.89% examples, 727 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 10:59:59,258 : INFO : EPOCH 1 - PROGRESS: at 80.17% examples, 778 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:00,353 : INFO : EPOCH 1 - PROGRESS: at 84.49% examples, 773 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:01,389 : INFO : EPOCH 1 - PROGRESS: at 90.73% examples, 756 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:02,363 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-22 11:00:02,365 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-22 11:00:02,366 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-22 11:00:02,367 : INFO : EPOCH - 1 : training on 2083112 raw words (15179 effective words) took 16.8s, 904 effective words/s\n",
      "2021-02-22 11:00:03,403 : INFO : EPOCH 2 - PROGRESS: at 5.76% examples, 292 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:04,424 : INFO : EPOCH 2 - PROGRESS: at 12.00% examples, 147 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:05,432 : INFO : EPOCH 2 - PROGRESS: at 19.68% examples, 179 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:06,451 : INFO : EPOCH 2 - PROGRESS: at 28.80% examples, 372 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:07,458 : INFO : EPOCH 2 - PROGRESS: at 35.52% examples, 492 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:08,500 : INFO : EPOCH 2 - PROGRESS: at 37.92% examples, 439 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:09,598 : INFO : EPOCH 2 - PROGRESS: at 45.60% examples, 917 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:10,697 : INFO : EPOCH 2 - PROGRESS: at 49.93% examples, 891 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:11,703 : INFO : EPOCH 2 - PROGRESS: at 54.25% examples, 861 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:12,746 : INFO : EPOCH 2 - PROGRESS: at 60.97% examples, 775 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:13,800 : INFO : EPOCH 2 - PROGRESS: at 68.17% examples, 717 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:15,003 : INFO : EPOCH 2 - PROGRESS: at 74.89% examples, 722 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:16,178 : INFO : EPOCH 2 - PROGRESS: at 80.17% examples, 767 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:17,235 : INFO : EPOCH 2 - PROGRESS: at 84.49% examples, 765 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:18,275 : INFO : EPOCH 2 - PROGRESS: at 90.73% examples, 748 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:00:19,221 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-22 11:00:19,225 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-22 11:00:19,226 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-22 11:00:19,226 : INFO : EPOCH - 2 : training on 2083112 raw words (15148 effective words) took 16.8s, 899 effective words/s\n",
      "2021-02-22 11:00:19,227 : INFO : training on a 4166224 raw words (30327 effective words) took 33.7s, 901 effective words/s\n",
      "2021-02-22 11:00:21,558 : INFO : saving FastText object under ./noteevents_clean_fasttext_win10_min5.dat, separately None\n",
      "2021-02-22 11:00:21,559 : INFO : storing np array 'vectors_ngrams' to ./noteevents_clean_fasttext_win10_min5.dat.wv.vectors_ngrams.npy\n",
      "2021-02-22 11:00:23,183 : INFO : not storing attribute vectors_norm\n",
      "2021-02-22 11:00:23,184 : INFO : not storing attribute vectors_vocab_norm\n",
      "2021-02-22 11:00:23,184 : INFO : not storing attribute vectors_ngrams_norm\n",
      "2021-02-22 11:00:23,185 : INFO : not storing attribute buckets_word\n",
      "2021-02-22 11:00:23,186 : INFO : storing np array 'vectors_ngrams_lockf' to ./noteevents_clean_fasttext_win10_min5.dat.trainables.vectors_ngrams_lockf.npy\n",
      "2021-02-22 11:00:24,810 : INFO : saved ./noteevents_clean_fasttext_win10_min5.dat\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND SAVE FASTTEXT\n",
    "ft_model = FastText(size=size, window=window, min_count=min_count)  # instantiate\n",
    "ft_model.build_vocab(sentences=sentences)\n",
    "ft_model.train(sentences=sentences, total_examples=sum(1 for _ in sentences), epochs=2)\n",
    "#ft_model.save(filename.replace('.txt','_fasttext_win10_min5.dat').replace('_tkns',''))\n",
    "ft_model.save(filename.replace('.txt','_fasttext_win10_min5.dat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2021-02-22 10:59:28,104 : INFO : collected 2019963 word types from a corpus of 2083112 raw words and 2083112 sentences\n",
    "2021-02-22 10:59:28,104 : INFO : Loading a fresh vocabulary\n",
    "2021-02-22 10:59:28,901 : INFO : effective_min_count=5 retains 1176 unique words (0% of original 2019963, drops 2018787)\n",
    "2021-02-22 10:59:28,902 : INFO : effective_min_count=5 leaves 23371 word corpus (1% of original 2083112, drops 2059741)\n",
    "2021-02-22 10:59:28,906 : INFO : deleting the raw counts dictionary of 2019963 items\n",
    "2021-02-22 10:59:29,173 : INFO : sample=0.001 downsamples 38 most-common words\n",
    "2021-02-22 10:59:29,174 : INFO : downsampling leaves estimated 15132 word corpus (64.7% of prior 23371)\n",
    "2021-02-22 10:59:29,656 : INFO : estimated required memory for 1176 words, 78129 buckets and 300 dimensions: 110096912 bytes\n",
    "2021-02-22 10:59:29,658 : INFO : resetting layer weights\n",
    "2021-02-22 10:59:45,575 : INFO : training model with 3 workers on 1176 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
    "2021-02-22 11:00:02,367 : INFO : EPOCH - 1 : training on 2083112 raw words (15179 effective words) took 16.8s, 904 effective words/s\n",
    "2021-02-22 11:00:19,226 : INFO : EPOCH - 2 : training on 2083112 raw words (15148 effective words) took 16.8s, 899 effective words/s\n",
    "2021-02-22 11:00:19,227 : INFO : training on a 4166224 raw words (30327 effective words) took 33.7s, 901 effective words/s\n",
    "2021-02-22 11:00:21,558 : INFO : saving FastText object under ./noteevents_clean_fasttext_win10_min5.dat, separately None\n",
    "2021-02-22 11:00:21,559 : INFO : storing np array 'vectors_ngrams' to ./noteevents_clean_fasttext_win10_min5.dat.wv.vectors_ngrams.npy\n",
    "2021-02-22 11:00:23,183 : INFO : not storing attribute vectors_norm\n",
    "2021-02-22 11:00:23,184 : INFO : not storing attribute vectors_vocab_norm\n",
    "2021-02-22 11:00:23,184 : INFO : not storing attribute vectors_ngrams_norm\n",
    "2021-02-22 11:00:23,185 : INFO : not storing attribute buckets_word\n",
    "2021-02-22 11:00:23,186 : INFO : storing np array 'vectors_ngrams_lockf' to ./noteevents_clean_fasttext_win10_min5.dat.trainables.vectors_ngrams_lockf.npy\n",
    "2021-02-22 11:00:24,810 : INFO : saved ./noteevents_clean_fasttext_win10_min5.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AswCiUxHAGi"
   },
   "source": [
    "## TRAIN W2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6EDXoD9nFtrX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 11:04:33,865 : INFO : collecting all words and their counts\n",
      "2021-02-22 11:04:33,868 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-02-22 11:04:34,021 : INFO : PROGRESS: at sentence #10000, processed 10000 words, keeping 9349 word types\n",
      "2021-02-22 11:04:34,140 : INFO : PROGRESS: at sentence #20000, processed 20000 words, keeping 18556 word types\n",
      "2021-02-22 11:04:34,254 : INFO : PROGRESS: at sentence #30000, processed 30000 words, keeping 27752 word types\n",
      "2021-02-22 11:04:34,342 : INFO : PROGRESS: at sentence #40000, processed 40000 words, keeping 37343 word types\n",
      "2021-02-22 11:04:34,401 : INFO : PROGRESS: at sentence #50000, processed 50000 words, keeping 47343 word types\n",
      "2021-02-22 11:04:34,461 : INFO : PROGRESS: at sentence #60000, processed 60000 words, keeping 57343 word types\n",
      "2021-02-22 11:04:34,518 : INFO : PROGRESS: at sentence #70000, processed 70000 words, keeping 67343 word types\n",
      "2021-02-22 11:04:34,582 : INFO : PROGRESS: at sentence #80000, processed 80000 words, keeping 77343 word types\n",
      "2021-02-22 11:04:34,641 : INFO : PROGRESS: at sentence #90000, processed 90000 words, keeping 87343 word types\n",
      "2021-02-22 11:04:34,703 : INFO : PROGRESS: at sentence #100000, processed 100000 words, keeping 97335 word types\n",
      "2021-02-22 11:04:34,763 : INFO : PROGRESS: at sentence #110000, processed 110000 words, keeping 107335 word types\n",
      "2021-02-22 11:04:34,823 : INFO : PROGRESS: at sentence #120000, processed 120000 words, keeping 117335 word types\n",
      "2021-02-22 11:04:34,883 : INFO : PROGRESS: at sentence #130000, processed 130000 words, keeping 127335 word types\n",
      "2021-02-22 11:04:34,943 : INFO : PROGRESS: at sentence #140000, processed 140000 words, keeping 137335 word types\n",
      "2021-02-22 11:04:35,003 : INFO : PROGRESS: at sentence #150000, processed 150000 words, keeping 147335 word types\n",
      "2021-02-22 11:04:35,064 : INFO : PROGRESS: at sentence #160000, processed 160000 words, keeping 157335 word types\n",
      "2021-02-22 11:04:35,125 : INFO : PROGRESS: at sentence #170000, processed 170000 words, keeping 167335 word types\n",
      "2021-02-22 11:04:35,188 : INFO : PROGRESS: at sentence #180000, processed 180000 words, keeping 177335 word types\n",
      "2021-02-22 11:04:35,246 : INFO : PROGRESS: at sentence #190000, processed 190000 words, keeping 187335 word types\n",
      "2021-02-22 11:04:35,305 : INFO : PROGRESS: at sentence #200000, processed 200000 words, keeping 197335 word types\n",
      "2021-02-22 11:04:35,366 : INFO : PROGRESS: at sentence #210000, processed 210000 words, keeping 207335 word types\n",
      "2021-02-22 11:04:35,429 : INFO : PROGRESS: at sentence #220000, processed 220000 words, keeping 217335 word types\n",
      "2021-02-22 11:04:35,493 : INFO : PROGRESS: at sentence #230000, processed 230000 words, keeping 227335 word types\n",
      "2021-02-22 11:04:35,556 : INFO : PROGRESS: at sentence #240000, processed 240000 words, keeping 237335 word types\n",
      "2021-02-22 11:04:35,620 : INFO : PROGRESS: at sentence #250000, processed 250000 words, keeping 247335 word types\n",
      "2021-02-22 11:04:35,687 : INFO : PROGRESS: at sentence #260000, processed 260000 words, keeping 257335 word types\n",
      "2021-02-22 11:04:35,753 : INFO : PROGRESS: at sentence #270000, processed 270000 words, keeping 267335 word types\n",
      "2021-02-22 11:04:35,819 : INFO : PROGRESS: at sentence #280000, processed 280000 words, keeping 277335 word types\n",
      "2021-02-22 11:04:35,884 : INFO : PROGRESS: at sentence #290000, processed 290000 words, keeping 287335 word types\n",
      "2021-02-22 11:04:35,943 : INFO : PROGRESS: at sentence #300000, processed 300000 words, keeping 297329 word types\n",
      "2021-02-22 11:04:35,997 : INFO : PROGRESS: at sentence #310000, processed 310000 words, keeping 307302 word types\n",
      "2021-02-22 11:04:36,043 : INFO : PROGRESS: at sentence #320000, processed 320000 words, keeping 317264 word types\n",
      "2021-02-22 11:04:36,092 : INFO : PROGRESS: at sentence #330000, processed 330000 words, keeping 327237 word types\n",
      "2021-02-22 11:04:36,138 : INFO : PROGRESS: at sentence #340000, processed 340000 words, keeping 337201 word types\n",
      "2021-02-22 11:04:36,185 : INFO : PROGRESS: at sentence #350000, processed 350000 words, keeping 347146 word types\n",
      "2021-02-22 11:04:36,241 : INFO : PROGRESS: at sentence #360000, processed 360000 words, keeping 357101 word types\n",
      "2021-02-22 11:04:36,286 : INFO : PROGRESS: at sentence #370000, processed 370000 words, keeping 367048 word types\n",
      "2021-02-22 11:04:36,331 : INFO : PROGRESS: at sentence #380000, processed 380000 words, keeping 377006 word types\n",
      "2021-02-22 11:04:36,373 : INFO : PROGRESS: at sentence #390000, processed 390000 words, keeping 386928 word types\n",
      "2021-02-22 11:04:36,417 : INFO : PROGRESS: at sentence #400000, processed 400000 words, keeping 396884 word types\n",
      "2021-02-22 11:04:36,464 : INFO : PROGRESS: at sentence #410000, processed 410000 words, keeping 406832 word types\n",
      "2021-02-22 11:04:36,511 : INFO : PROGRESS: at sentence #420000, processed 420000 words, keeping 416784 word types\n",
      "2021-02-22 11:04:36,559 : INFO : PROGRESS: at sentence #430000, processed 430000 words, keeping 426723 word types\n",
      "2021-02-22 11:04:36,606 : INFO : PROGRESS: at sentence #440000, processed 440000 words, keeping 436668 word types\n",
      "2021-02-22 11:04:36,653 : INFO : PROGRESS: at sentence #450000, processed 450000 words, keeping 446622 word types\n",
      "2021-02-22 11:04:36,697 : INFO : PROGRESS: at sentence #460000, processed 460000 words, keeping 456565 word types\n",
      "2021-02-22 11:04:36,741 : INFO : PROGRESS: at sentence #470000, processed 470000 words, keeping 466520 word types\n",
      "2021-02-22 11:04:36,785 : INFO : PROGRESS: at sentence #480000, processed 480000 words, keeping 476460 word types\n",
      "2021-02-22 11:04:36,832 : INFO : PROGRESS: at sentence #490000, processed 490000 words, keeping 486384 word types\n",
      "2021-02-22 11:04:36,880 : INFO : PROGRESS: at sentence #500000, processed 500000 words, keeping 496291 word types\n",
      "2021-02-22 11:04:36,922 : INFO : PROGRESS: at sentence #510000, processed 510000 words, keeping 506182 word types\n",
      "2021-02-22 11:04:36,963 : INFO : PROGRESS: at sentence #520000, processed 520000 words, keeping 516046 word types\n",
      "2021-02-22 11:04:36,999 : INFO : PROGRESS: at sentence #530000, processed 530000 words, keeping 525884 word types\n",
      "2021-02-22 11:04:37,033 : INFO : PROGRESS: at sentence #540000, processed 540000 words, keeping 535710 word types\n",
      "2021-02-22 11:04:37,066 : INFO : PROGRESS: at sentence #550000, processed 550000 words, keeping 545525 word types\n",
      "2021-02-22 11:04:37,099 : INFO : PROGRESS: at sentence #560000, processed 560000 words, keeping 555358 word types\n",
      "2021-02-22 11:04:37,132 : INFO : PROGRESS: at sentence #570000, processed 570000 words, keeping 565158 word types\n",
      "2021-02-22 11:04:37,169 : INFO : PROGRESS: at sentence #580000, processed 580000 words, keeping 574934 word types\n",
      "2021-02-22 11:04:37,203 : INFO : PROGRESS: at sentence #590000, processed 590000 words, keeping 584713 word types\n",
      "2021-02-22 11:04:37,234 : INFO : PROGRESS: at sentence #600000, processed 600000 words, keeping 594530 word types\n",
      "2021-02-22 11:04:37,267 : INFO : PROGRESS: at sentence #610000, processed 610000 words, keeping 604346 word types\n",
      "2021-02-22 11:04:37,299 : INFO : PROGRESS: at sentence #620000, processed 620000 words, keeping 614108 word types\n",
      "2021-02-22 11:04:37,333 : INFO : PROGRESS: at sentence #630000, processed 630000 words, keeping 623921 word types\n",
      "2021-02-22 11:04:37,371 : INFO : PROGRESS: at sentence #640000, processed 640000 words, keeping 633714 word types\n",
      "2021-02-22 11:04:37,405 : INFO : PROGRESS: at sentence #650000, processed 650000 words, keeping 643489 word types\n",
      "2021-02-22 11:04:37,440 : INFO : PROGRESS: at sentence #660000, processed 660000 words, keeping 653292 word types\n",
      "2021-02-22 11:04:37,474 : INFO : PROGRESS: at sentence #670000, processed 670000 words, keeping 663081 word types\n",
      "2021-02-22 11:04:37,508 : INFO : PROGRESS: at sentence #680000, processed 680000 words, keeping 672801 word types\n",
      "2021-02-22 11:04:37,541 : INFO : PROGRESS: at sentence #690000, processed 690000 words, keeping 682553 word types\n",
      "2021-02-22 11:04:37,576 : INFO : PROGRESS: at sentence #700000, processed 700000 words, keeping 692346 word types\n",
      "2021-02-22 11:04:37,634 : INFO : PROGRESS: at sentence #710000, processed 710000 words, keeping 702170 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 11:04:37,680 : INFO : PROGRESS: at sentence #720000, processed 720000 words, keeping 712016 word types\n",
      "2021-02-22 11:04:37,729 : INFO : PROGRESS: at sentence #730000, processed 730000 words, keeping 721827 word types\n",
      "2021-02-22 11:04:37,958 : INFO : PROGRESS: at sentence #740000, processed 740000 words, keeping 731787 word types\n",
      "2021-02-22 11:04:38,238 : INFO : PROGRESS: at sentence #750000, processed 750000 words, keeping 741771 word types\n",
      "2021-02-22 11:04:38,536 : INFO : PROGRESS: at sentence #760000, processed 760000 words, keeping 751761 word types\n",
      "2021-02-22 11:04:38,620 : INFO : PROGRESS: at sentence #770000, processed 770000 words, keeping 761756 word types\n",
      "2021-02-22 11:04:38,707 : INFO : PROGRESS: at sentence #780000, processed 780000 words, keeping 771742 word types\n",
      "2021-02-22 11:04:38,765 : INFO : PROGRESS: at sentence #790000, processed 790000 words, keeping 781413 word types\n",
      "2021-02-22 11:04:38,789 : INFO : PROGRESS: at sentence #800000, processed 800000 words, keeping 790809 word types\n",
      "2021-02-22 11:04:38,811 : INFO : PROGRESS: at sentence #810000, processed 810000 words, keeping 800253 word types\n",
      "2021-02-22 11:04:38,834 : INFO : PROGRESS: at sentence #820000, processed 820000 words, keeping 809679 word types\n",
      "2021-02-22 11:04:38,854 : INFO : PROGRESS: at sentence #830000, processed 830000 words, keeping 819020 word types\n",
      "2021-02-22 11:04:38,877 : INFO : PROGRESS: at sentence #840000, processed 840000 words, keeping 828299 word types\n",
      "2021-02-22 11:04:38,897 : INFO : PROGRESS: at sentence #850000, processed 850000 words, keeping 837363 word types\n",
      "2021-02-22 11:04:38,919 : INFO : PROGRESS: at sentence #860000, processed 860000 words, keeping 846463 word types\n",
      "2021-02-22 11:04:38,941 : INFO : PROGRESS: at sentence #870000, processed 870000 words, keeping 855672 word types\n",
      "2021-02-22 11:04:38,965 : INFO : PROGRESS: at sentence #880000, processed 880000 words, keeping 864956 word types\n",
      "2021-02-22 11:04:39,004 : INFO : PROGRESS: at sentence #890000, processed 890000 words, keeping 874076 word types\n",
      "2021-02-22 11:04:39,091 : INFO : PROGRESS: at sentence #900000, processed 900000 words, keeping 883306 word types\n",
      "2021-02-22 11:04:39,197 : INFO : PROGRESS: at sentence #910000, processed 910000 words, keeping 892451 word types\n",
      "2021-02-22 11:04:39,304 : INFO : PROGRESS: at sentence #920000, processed 920000 words, keeping 901573 word types\n",
      "2021-02-22 11:04:39,415 : INFO : PROGRESS: at sentence #930000, processed 930000 words, keeping 910768 word types\n",
      "2021-02-22 11:04:39,525 : INFO : PROGRESS: at sentence #940000, processed 940000 words, keeping 919993 word types\n",
      "2021-02-22 11:04:39,625 : INFO : PROGRESS: at sentence #950000, processed 950000 words, keeping 929149 word types\n",
      "2021-02-22 11:04:39,729 : INFO : PROGRESS: at sentence #960000, processed 960000 words, keeping 938368 word types\n",
      "2021-02-22 11:04:39,831 : INFO : PROGRESS: at sentence #970000, processed 970000 words, keeping 947550 word types\n",
      "2021-02-22 11:04:39,937 : INFO : PROGRESS: at sentence #980000, processed 980000 words, keeping 956811 word types\n",
      "2021-02-22 11:04:40,041 : INFO : PROGRESS: at sentence #990000, processed 990000 words, keeping 966010 word types\n",
      "2021-02-22 11:04:40,145 : INFO : PROGRESS: at sentence #1000000, processed 1000000 words, keeping 975148 word types\n",
      "2021-02-22 11:04:40,249 : INFO : PROGRESS: at sentence #1010000, processed 1010000 words, keeping 984277 word types\n",
      "2021-02-22 11:04:40,349 : INFO : PROGRESS: at sentence #1020000, processed 1020000 words, keeping 993446 word types\n",
      "2021-02-22 11:04:40,453 : INFO : PROGRESS: at sentence #1030000, processed 1030000 words, keeping 1002641 word types\n",
      "2021-02-22 11:04:40,553 : INFO : PROGRESS: at sentence #1040000, processed 1040000 words, keeping 1011914 word types\n",
      "2021-02-22 11:04:40,660 : INFO : PROGRESS: at sentence #1050000, processed 1050000 words, keeping 1021082 word types\n",
      "2021-02-22 11:04:40,763 : INFO : PROGRESS: at sentence #1060000, processed 1060000 words, keeping 1030162 word types\n",
      "2021-02-22 11:04:40,860 : INFO : PROGRESS: at sentence #1070000, processed 1070000 words, keeping 1039329 word types\n",
      "2021-02-22 11:04:40,965 : INFO : PROGRESS: at sentence #1080000, processed 1080000 words, keeping 1048532 word types\n",
      "2021-02-22 11:04:41,058 : INFO : PROGRESS: at sentence #1090000, processed 1090000 words, keeping 1057783 word types\n",
      "2021-02-22 11:04:41,152 : INFO : PROGRESS: at sentence #1100000, processed 1100000 words, keeping 1066941 word types\n",
      "2021-02-22 11:04:41,244 : INFO : PROGRESS: at sentence #1110000, processed 1110000 words, keeping 1076149 word types\n",
      "2021-02-22 11:04:41,299 : INFO : PROGRESS: at sentence #1120000, processed 1120000 words, keeping 1086104 word types\n",
      "2021-02-22 11:04:41,352 : INFO : PROGRESS: at sentence #1130000, processed 1130000 words, keeping 1096104 word types\n",
      "2021-02-22 11:04:41,404 : INFO : PROGRESS: at sentence #1140000, processed 1140000 words, keeping 1106104 word types\n",
      "2021-02-22 11:04:41,460 : INFO : PROGRESS: at sentence #1150000, processed 1150000 words, keeping 1116104 word types\n",
      "2021-02-22 11:04:41,515 : INFO : PROGRESS: at sentence #1160000, processed 1160000 words, keeping 1126104 word types\n",
      "2021-02-22 11:04:41,569 : INFO : PROGRESS: at sentence #1170000, processed 1170000 words, keeping 1136104 word types\n",
      "2021-02-22 11:04:41,623 : INFO : PROGRESS: at sentence #1180000, processed 1180000 words, keeping 1146104 word types\n",
      "2021-02-22 11:04:41,681 : INFO : PROGRESS: at sentence #1190000, processed 1190000 words, keeping 1156104 word types\n",
      "2021-02-22 11:04:41,736 : INFO : PROGRESS: at sentence #1200000, processed 1200000 words, keeping 1166104 word types\n",
      "2021-02-22 11:04:41,791 : INFO : PROGRESS: at sentence #1210000, processed 1210000 words, keeping 1176104 word types\n",
      "2021-02-22 11:04:41,849 : INFO : PROGRESS: at sentence #1220000, processed 1220000 words, keeping 1186104 word types\n",
      "2021-02-22 11:04:41,907 : INFO : PROGRESS: at sentence #1230000, processed 1230000 words, keeping 1196104 word types\n",
      "2021-02-22 11:04:41,964 : INFO : PROGRESS: at sentence #1240000, processed 1240000 words, keeping 1206104 word types\n",
      "2021-02-22 11:04:42,023 : INFO : PROGRESS: at sentence #1250000, processed 1250000 words, keeping 1216104 word types\n",
      "2021-02-22 11:04:42,082 : INFO : PROGRESS: at sentence #1260000, processed 1260000 words, keeping 1226103 word types\n",
      "2021-02-22 11:04:42,137 : INFO : PROGRESS: at sentence #1270000, processed 1270000 words, keeping 1236089 word types\n",
      "2021-02-22 11:04:42,183 : INFO : PROGRESS: at sentence #1280000, processed 1280000 words, keeping 1246042 word types\n",
      "2021-02-22 11:04:42,237 : INFO : PROGRESS: at sentence #1290000, processed 1290000 words, keeping 1256042 word types\n",
      "2021-02-22 11:04:42,287 : INFO : PROGRESS: at sentence #1300000, processed 1300000 words, keeping 1266042 word types\n",
      "2021-02-22 11:04:42,340 : INFO : PROGRESS: at sentence #1310000, processed 1310000 words, keeping 1276042 word types\n",
      "2021-02-22 11:04:42,393 : INFO : PROGRESS: at sentence #1320000, processed 1320000 words, keeping 1286042 word types\n",
      "2021-02-22 11:04:42,446 : INFO : PROGRESS: at sentence #1330000, processed 1330000 words, keeping 1296042 word types\n",
      "2021-02-22 11:04:42,500 : INFO : PROGRESS: at sentence #1340000, processed 1340000 words, keeping 1306042 word types\n",
      "2021-02-22 11:04:42,555 : INFO : PROGRESS: at sentence #1350000, processed 1350000 words, keeping 1316042 word types\n",
      "2021-02-22 11:04:42,608 : INFO : PROGRESS: at sentence #1360000, processed 1360000 words, keeping 1326042 word types\n",
      "2021-02-22 11:04:42,663 : INFO : PROGRESS: at sentence #1370000, processed 1370000 words, keeping 1336042 word types\n",
      "2021-02-22 11:04:42,713 : INFO : PROGRESS: at sentence #1380000, processed 1380000 words, keeping 1346010 word types\n",
      "2021-02-22 11:04:42,758 : INFO : PROGRESS: at sentence #1390000, processed 1390000 words, keeping 1355961 word types\n",
      "2021-02-22 11:04:42,800 : INFO : PROGRESS: at sentence #1400000, processed 1400000 words, keeping 1365893 word types\n",
      "2021-02-22 11:04:42,844 : INFO : PROGRESS: at sentence #1410000, processed 1410000 words, keeping 1375820 word types\n",
      "2021-02-22 11:04:42,890 : INFO : PROGRESS: at sentence #1420000, processed 1420000 words, keeping 1385731 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 11:04:42,932 : INFO : PROGRESS: at sentence #1430000, processed 1430000 words, keeping 1395642 word types\n",
      "2021-02-22 11:04:43,020 : INFO : PROGRESS: at sentence #1440000, processed 1440000 words, keeping 1405507 word types\n",
      "2021-02-22 11:04:43,053 : INFO : PROGRESS: at sentence #1450000, processed 1450000 words, keeping 1415287 word types\n",
      "2021-02-22 11:04:43,087 : INFO : PROGRESS: at sentence #1460000, processed 1460000 words, keeping 1425039 word types\n",
      "2021-02-22 11:04:43,120 : INFO : PROGRESS: at sentence #1470000, processed 1470000 words, keeping 1434785 word types\n",
      "2021-02-22 11:04:43,154 : INFO : PROGRESS: at sentence #1480000, processed 1480000 words, keeping 1444531 word types\n",
      "2021-02-22 11:04:43,192 : INFO : PROGRESS: at sentence #1490000, processed 1490000 words, keeping 1454304 word types\n",
      "2021-02-22 11:04:43,226 : INFO : PROGRESS: at sentence #1500000, processed 1500000 words, keeping 1464090 word types\n",
      "2021-02-22 11:04:43,261 : INFO : PROGRESS: at sentence #1510000, processed 1510000 words, keeping 1473804 word types\n",
      "2021-02-22 11:04:43,295 : INFO : PROGRESS: at sentence #1520000, processed 1520000 words, keeping 1483576 word types\n",
      "2021-02-22 11:04:43,332 : INFO : PROGRESS: at sentence #1530000, processed 1530000 words, keeping 1493369 word types\n",
      "2021-02-22 11:04:43,382 : INFO : PROGRESS: at sentence #1540000, processed 1540000 words, keeping 1503140 word types\n",
      "2021-02-22 11:04:43,622 : INFO : PROGRESS: at sentence #1550000, processed 1550000 words, keeping 1513127 word types\n",
      "2021-02-22 11:04:43,880 : INFO : PROGRESS: at sentence #1560000, processed 1560000 words, keeping 1523114 word types\n",
      "2021-02-22 11:04:44,110 : INFO : PROGRESS: at sentence #1570000, processed 1570000 words, keeping 1533099 word types\n",
      "2021-02-22 11:04:44,185 : INFO : PROGRESS: at sentence #1580000, processed 1580000 words, keeping 1543098 word types\n",
      "2021-02-22 11:04:44,260 : INFO : PROGRESS: at sentence #1590000, processed 1590000 words, keeping 1553010 word types\n",
      "2021-02-22 11:04:44,298 : INFO : PROGRESS: at sentence #1600000, processed 1600000 words, keeping 1562388 word types\n",
      "2021-02-22 11:04:44,319 : INFO : PROGRESS: at sentence #1610000, processed 1610000 words, keeping 1571600 word types\n",
      "2021-02-22 11:04:44,339 : INFO : PROGRESS: at sentence #1620000, processed 1620000 words, keeping 1580749 word types\n",
      "2021-02-22 11:04:44,361 : INFO : PROGRESS: at sentence #1630000, processed 1630000 words, keeping 1589947 word types\n",
      "2021-02-22 11:04:44,423 : INFO : PROGRESS: at sentence #1640000, processed 1640000 words, keeping 1599088 word types\n",
      "2021-02-22 11:04:44,515 : INFO : PROGRESS: at sentence #1650000, processed 1650000 words, keeping 1608230 word types\n",
      "2021-02-22 11:04:44,606 : INFO : PROGRESS: at sentence #1660000, processed 1660000 words, keeping 1617407 word types\n",
      "2021-02-22 11:04:44,702 : INFO : PROGRESS: at sentence #1670000, processed 1670000 words, keeping 1626617 word types\n",
      "2021-02-22 11:04:44,794 : INFO : PROGRESS: at sentence #1680000, processed 1680000 words, keeping 1635665 word types\n",
      "2021-02-22 11:04:44,881 : INFO : PROGRESS: at sentence #1690000, processed 1690000 words, keeping 1644827 word types\n",
      "2021-02-22 11:04:44,970 : INFO : PROGRESS: at sentence #1700000, processed 1700000 words, keeping 1654060 word types\n",
      "2021-02-22 11:04:45,062 : INFO : PROGRESS: at sentence #1710000, processed 1710000 words, keeping 1663268 word types\n",
      "2021-02-22 11:04:45,156 : INFO : PROGRESS: at sentence #1720000, processed 1720000 words, keeping 1672429 word types\n",
      "2021-02-22 11:04:45,251 : INFO : PROGRESS: at sentence #1730000, processed 1730000 words, keeping 1681571 word types\n",
      "2021-02-22 11:04:45,342 : INFO : PROGRESS: at sentence #1740000, processed 1740000 words, keeping 1690652 word types\n",
      "2021-02-22 11:04:45,437 : INFO : PROGRESS: at sentence #1750000, processed 1750000 words, keeping 1699887 word types\n",
      "2021-02-22 11:04:45,529 : INFO : PROGRESS: at sentence #1760000, processed 1760000 words, keeping 1709056 word types\n",
      "2021-02-22 11:04:45,622 : INFO : PROGRESS: at sentence #1770000, processed 1770000 words, keeping 1718223 word types\n",
      "2021-02-22 11:04:45,714 : INFO : PROGRESS: at sentence #1780000, processed 1780000 words, keeping 1727328 word types\n",
      "2021-02-22 11:04:45,803 : INFO : PROGRESS: at sentence #1790000, processed 1790000 words, keeping 1736504 word types\n",
      "2021-02-22 11:04:45,870 : INFO : PROGRESS: at sentence #1800000, processed 1800000 words, keeping 1746043 word types\n",
      "2021-02-22 11:04:45,914 : INFO : PROGRESS: at sentence #1810000, processed 1810000 words, keeping 1755986 word types\n",
      "2021-02-22 11:04:45,955 : INFO : PROGRESS: at sentence #1820000, processed 1820000 words, keeping 1765927 word types\n",
      "2021-02-22 11:04:45,997 : INFO : PROGRESS: at sentence #1830000, processed 1830000 words, keeping 1775854 word types\n",
      "2021-02-22 11:04:46,038 : INFO : PROGRESS: at sentence #1840000, processed 1840000 words, keeping 1785786 word types\n",
      "2021-02-22 11:04:46,079 : INFO : PROGRESS: at sentence #1850000, processed 1850000 words, keeping 1795728 word types\n",
      "2021-02-22 11:04:46,120 : INFO : PROGRESS: at sentence #1860000, processed 1860000 words, keeping 1805675 word types\n",
      "2021-02-22 11:04:46,161 : INFO : PROGRESS: at sentence #1870000, processed 1870000 words, keeping 1815617 word types\n",
      "2021-02-22 11:04:46,204 : INFO : PROGRESS: at sentence #1880000, processed 1880000 words, keeping 1825552 word types\n",
      "2021-02-22 11:04:46,244 : INFO : PROGRESS: at sentence #1890000, processed 1890000 words, keeping 1835478 word types\n",
      "2021-02-22 11:04:46,286 : INFO : PROGRESS: at sentence #1900000, processed 1900000 words, keeping 1845409 word types\n",
      "2021-02-22 11:04:46,327 : INFO : PROGRESS: at sentence #1910000, processed 1910000 words, keeping 1855346 word types\n",
      "2021-02-22 11:04:46,369 : INFO : PROGRESS: at sentence #1920000, processed 1920000 words, keeping 1865294 word types\n",
      "2021-02-22 11:04:46,409 : INFO : PROGRESS: at sentence #1930000, processed 1930000 words, keeping 1875221 word types\n",
      "2021-02-22 11:04:46,446 : INFO : PROGRESS: at sentence #1940000, processed 1940000 words, keeping 1885007 word types\n",
      "2021-02-22 11:04:46,481 : INFO : PROGRESS: at sentence #1950000, processed 1950000 words, keeping 1894742 word types\n",
      "2021-02-22 11:04:46,519 : INFO : PROGRESS: at sentence #1960000, processed 1960000 words, keeping 1904444 word types\n",
      "2021-02-22 11:04:46,557 : INFO : PROGRESS: at sentence #1970000, processed 1970000 words, keeping 1914147 word types\n",
      "2021-02-22 11:04:46,595 : INFO : PROGRESS: at sentence #1980000, processed 1980000 words, keeping 1923877 word types\n",
      "2021-02-22 11:04:46,633 : INFO : PROGRESS: at sentence #1990000, processed 1990000 words, keeping 1933557 word types\n",
      "2021-02-22 11:04:46,672 : INFO : PROGRESS: at sentence #2000000, processed 2000000 words, keeping 1943329 word types\n",
      "2021-02-22 11:04:46,722 : INFO : PROGRESS: at sentence #2010000, processed 2010000 words, keeping 1953058 word types\n",
      "2021-02-22 11:04:46,761 : INFO : PROGRESS: at sentence #2020000, processed 2020000 words, keeping 1962584 word types\n",
      "2021-02-22 11:04:46,782 : INFO : PROGRESS: at sentence #2030000, processed 2030000 words, keeping 1971747 word types\n",
      "2021-02-22 11:04:46,802 : INFO : PROGRESS: at sentence #2040000, processed 2040000 words, keeping 1980887 word types\n",
      "2021-02-22 11:04:46,822 : INFO : PROGRESS: at sentence #2050000, processed 2050000 words, keeping 1989929 word types\n",
      "2021-02-22 11:04:46,844 : INFO : PROGRESS: at sentence #2060000, processed 2060000 words, keeping 1998855 word types\n",
      "2021-02-22 11:04:46,865 : INFO : PROGRESS: at sentence #2070000, processed 2070000 words, keeping 2007829 word types\n",
      "2021-02-22 11:04:46,926 : INFO : PROGRESS: at sentence #2080000, processed 2080000 words, keeping 2017081 word types\n",
      "2021-02-22 11:04:46,937 : INFO : collected 2019963 word types from a corpus of 2083112 raw words and 2083112 sentences\n",
      "2021-02-22 11:04:46,938 : INFO : Loading a fresh vocabulary\n",
      "2021-02-22 11:04:47,731 : INFO : effective_min_count=5 retains 1176 unique words (0% of original 2019963, drops 2018787)\n",
      "2021-02-22 11:04:47,732 : INFO : effective_min_count=5 leaves 23371 word corpus (1% of original 2083112, drops 2059741)\n",
      "2021-02-22 11:04:47,736 : INFO : deleting the raw counts dictionary of 2019963 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 11:04:47,993 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2021-02-22 11:04:47,994 : INFO : downsampling leaves estimated 15132 word corpus (64.7% of prior 23371)\n",
      "2021-02-22 11:04:47,997 : INFO : estimated required memory for 1176 words and 300 dimensions: 3410400 bytes\n",
      "2021-02-22 11:04:47,998 : INFO : resetting layer weights\n",
      "2021-02-22 11:04:48,240 : INFO : training model with 16 workers on 1176 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2021-02-22 11:04:49,249 : INFO : EPOCH 1 - PROGRESS: at 4.80% examples, 303 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:04:50,252 : INFO : EPOCH 1 - PROGRESS: at 11.04% examples, 152 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:04:51,266 : INFO : EPOCH 1 - PROGRESS: at 17.28% examples, 143 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:04:52,388 : INFO : EPOCH 1 - PROGRESS: at 25.92% examples, 257 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:04:53,476 : INFO : EPOCH 1 - PROGRESS: at 35.04% examples, 482 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:04:54,528 : INFO : EPOCH 1 - PROGRESS: at 36.96% examples, 403 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:04:55,626 : INFO : EPOCH 1 - PROGRESS: at 44.64% examples, 885 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:04:56,761 : INFO : EPOCH 1 - PROGRESS: at 48.97% examples, 856 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:04:57,839 : INFO : EPOCH 1 - PROGRESS: at 53.29% examples, 841 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:04:58,863 : INFO : EPOCH 1 - PROGRESS: at 59.05% examples, 760 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:04:59,882 : INFO : EPOCH 1 - PROGRESS: at 65.29% examples, 695 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:00,888 : INFO : EPOCH 1 - PROGRESS: at 72.49% examples, 704 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:01,923 : INFO : EPOCH 1 - PROGRESS: at 75.37% examples, 669 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:02,924 : INFO : EPOCH 1 - PROGRESS: at 80.65% examples, 728 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:04,018 : INFO : EPOCH 1 - PROGRESS: at 84.49% examples, 722 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:05,097 : INFO : EPOCH 1 - PROGRESS: at 90.25% examples, 704 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:06,102 : INFO : EPOCH 1 - PROGRESS: at 98.41% examples, 787 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:06,341 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-02-22 11:05:06,359 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-02-22 11:05:06,361 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-02-22 11:05:06,362 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-02-22 11:05:06,362 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-02-22 11:05:06,363 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-02-22 11:05:06,363 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-02-22 11:05:06,364 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-02-22 11:05:06,365 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-02-22 11:05:06,365 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-02-22 11:05:06,366 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-02-22 11:05:06,366 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-02-22 11:05:06,367 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-22 11:05:06,367 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-22 11:05:06,368 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-22 11:05:06,368 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-22 11:05:06,369 : INFO : EPOCH - 1 : training on 2083112 raw words (15121 effective words) took 18.1s, 834 effective words/s\n",
      "2021-02-22 11:05:07,379 : INFO : EPOCH 2 - PROGRESS: at 5.76% examples, 294 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:08,448 : INFO : EPOCH 2 - PROGRESS: at 12.48% examples, 142 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:09,454 : INFO : EPOCH 2 - PROGRESS: at 19.20% examples, 171 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:10,499 : INFO : EPOCH 2 - PROGRESS: at 27.36% examples, 312 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:11,597 : INFO : EPOCH 2 - PROGRESS: at 35.52% examples, 480 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:12,672 : INFO : EPOCH 2 - PROGRESS: at 37.92% examples, 427 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:13,867 : INFO : EPOCH 2 - PROGRESS: at 45.60% examples, 884 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:14,880 : INFO : EPOCH 2 - PROGRESS: at 49.93% examples, 871 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:15,933 : INFO : EPOCH 2 - PROGRESS: at 53.77% examples, 840 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:16,971 : INFO : EPOCH 2 - PROGRESS: at 59.53% examples, 757 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:18,013 : INFO : EPOCH 2 - PROGRESS: at 65.77% examples, 692 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:19,071 : INFO : EPOCH 2 - PROGRESS: at 73.45% examples, 710 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:20,077 : INFO : EPOCH 2 - PROGRESS: at 75.85% examples, 665 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:21,204 : INFO : EPOCH 2 - PROGRESS: at 81.61% examples, 734 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:22,276 : INFO : EPOCH 2 - PROGRESS: at 85.45% examples, 727 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:23,355 : INFO : EPOCH 2 - PROGRESS: at 92.17% examples, 704 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:24,313 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-02-22 11:05:24,328 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-02-22 11:05:24,334 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-02-22 11:05:24,355 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-02-22 11:05:24,355 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-02-22 11:05:24,356 : INFO : EPOCH 2 - PROGRESS: at 100.00% examples, 842 words/s, in_qsize 0, out_qsize 11\n",
      "2021-02-22 11:05:24,357 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-02-22 11:05:24,357 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-02-22 11:05:24,358 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-02-22 11:05:24,358 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-02-22 11:05:24,359 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-02-22 11:05:24,359 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-02-22 11:05:24,360 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-02-22 11:05:24,361 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-22 11:05:24,361 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-22 11:05:24,362 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-22 11:05:24,362 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-22 11:05:24,363 : INFO : EPOCH - 2 : training on 2083112 raw words (15131 effective words) took 18.0s, 841 effective words/s\n",
      "2021-02-22 11:05:25,457 : INFO : EPOCH 3 - PROGRESS: at 5.76% examples, 273 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:26,458 : INFO : EPOCH 3 - PROGRESS: at 12.00% examples, 142 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:27,467 : INFO : EPOCH 3 - PROGRESS: at 19.20% examples, 170 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:28,511 : INFO : EPOCH 3 - PROGRESS: at 27.36% examples, 311 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:29,698 : INFO : EPOCH 3 - PROGRESS: at 35.52% examples, 472 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:30,748 : INFO : EPOCH 3 - PROGRESS: at 37.92% examples, 423 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:31,769 : INFO : EPOCH 3 - PROGRESS: at 45.12% examples, 881 words/s, in_qsize 0, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 11:05:32,867 : INFO : EPOCH 3 - PROGRESS: at 49.45% examples, 859 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:33,944 : INFO : EPOCH 3 - PROGRESS: at 53.29% examples, 834 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:34,953 : INFO : EPOCH 3 - PROGRESS: at 59.53% examples, 754 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:36,032 : INFO : EPOCH 3 - PROGRESS: at 65.77% examples, 687 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:37,033 : INFO : EPOCH 3 - PROGRESS: at 73.93% examples, 716 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:38,100 : INFO : EPOCH 3 - PROGRESS: at 75.85% examples, 660 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:39,109 : INFO : EPOCH 3 - PROGRESS: at 81.13% examples, 729 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:40,193 : INFO : EPOCH 3 - PROGRESS: at 84.97% examples, 722 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:41,194 : INFO : EPOCH 3 - PROGRESS: at 91.21% examples, 705 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:42,195 : INFO : EPOCH 3 - PROGRESS: at 98.41% examples, 785 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:42,334 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-02-22 11:05:42,372 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-02-22 11:05:42,374 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-02-22 11:05:42,375 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-02-22 11:05:42,375 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-02-22 11:05:42,376 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-02-22 11:05:42,377 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-02-22 11:05:42,377 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-02-22 11:05:42,378 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-02-22 11:05:42,379 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-02-22 11:05:42,379 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-02-22 11:05:42,380 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-02-22 11:05:42,381 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-22 11:05:42,381 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-22 11:05:42,382 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-22 11:05:42,383 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-22 11:05:42,383 : INFO : EPOCH - 3 : training on 2083112 raw words (15072 effective words) took 18.0s, 837 effective words/s\n",
      "2021-02-22 11:05:43,451 : INFO : EPOCH 4 - PROGRESS: at 6.24% examples, 276 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:44,482 : INFO : EPOCH 4 - PROGRESS: at 12.48% examples, 140 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:45,508 : INFO : EPOCH 4 - PROGRESS: at 20.64% examples, 192 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:46,536 : INFO : EPOCH 4 - PROGRESS: at 29.28% examples, 386 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:47,799 : INFO : EPOCH 4 - PROGRESS: at 36.00% examples, 466 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:48,831 : INFO : EPOCH 4 - PROGRESS: at 42.24% examples, 918 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:49,922 : INFO : EPOCH 4 - PROGRESS: at 47.04% examples, 923 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:50,949 : INFO : EPOCH 4 - PROGRESS: at 50.89% examples, 896 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:51,959 : INFO : EPOCH 4 - PROGRESS: at 55.69% examples, 843 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:52,987 : INFO : EPOCH 4 - PROGRESS: at 61.93% examples, 764 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:54,020 : INFO : EPOCH 4 - PROGRESS: at 68.17% examples, 707 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:55,280 : INFO : EPOCH 4 - PROGRESS: at 74.89% examples, 709 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:56,284 : INFO : EPOCH 4 - PROGRESS: at 79.69% examples, 759 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:57,323 : INFO : EPOCH 4 - PROGRESS: at 83.53% examples, 752 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:58,369 : INFO : EPOCH 4 - PROGRESS: at 88.81% examples, 740 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:59,376 : INFO : EPOCH 4 - PROGRESS: at 96.01% examples, 745 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:05:59,825 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-02-22 11:05:59,856 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-02-22 11:05:59,862 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-02-22 11:05:59,876 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-02-22 11:05:59,877 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-02-22 11:05:59,878 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-02-22 11:05:59,879 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-02-22 11:05:59,879 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-02-22 11:05:59,880 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-02-22 11:05:59,881 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-02-22 11:05:59,881 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-02-22 11:05:59,882 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-02-22 11:05:59,883 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-22 11:05:59,883 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-22 11:05:59,884 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-22 11:05:59,884 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-22 11:05:59,885 : INFO : EPOCH - 4 : training on 2083112 raw words (15144 effective words) took 17.5s, 866 effective words/s\n",
      "2021-02-22 11:06:00,947 : INFO : EPOCH 5 - PROGRESS: at 6.24% examples, 276 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:06:02,067 : INFO : EPOCH 5 - PROGRESS: at 12.96% examples, 134 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:06:03,112 : INFO : EPOCH 5 - PROGRESS: at 21.12% examples, 197 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:06:04,115 : INFO : EPOCH 5 - PROGRESS: at 29.28% examples, 379 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:06:05,295 : INFO : EPOCH 5 - PROGRESS: at 36.00% examples, 465 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:06:06,363 : INFO : EPOCH 5 - PROGRESS: at 40.80% examples, 736 words/s, in_qsize 0, out_qsize 1\n",
      "2021-02-22 11:06:07,451 : INFO : EPOCH 5 - PROGRESS: at 46.56% examples, 900 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:06:08,567 : INFO : EPOCH 5 - PROGRESS: at 50.89% examples, 879 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:06:09,592 : INFO : EPOCH 5 - PROGRESS: at 55.69% examples, 827 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:06:10,621 : INFO : EPOCH 5 - PROGRESS: at 61.93% examples, 750 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:06:11,725 : INFO : EPOCH 5 - PROGRESS: at 68.65% examples, 695 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:06:12,924 : INFO : EPOCH 5 - PROGRESS: at 74.89% examples, 699 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:06:13,932 : INFO : EPOCH 5 - PROGRESS: at 79.69% examples, 749 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:06:15,007 : INFO : EPOCH 5 - PROGRESS: at 83.53% examples, 741 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:06:16,048 : INFO : EPOCH 5 - PROGRESS: at 88.33% examples, 729 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:06:17,168 : INFO : EPOCH 5 - PROGRESS: at 96.01% examples, 730 words/s, in_qsize 0, out_qsize 0\n",
      "2021-02-22 11:06:17,561 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-02-22 11:06:17,592 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-02-22 11:06:17,611 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-02-22 11:06:17,612 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-02-22 11:06:17,613 : INFO : worker thread finished; awaiting finish of 11 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-22 11:06:17,613 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-02-22 11:06:17,614 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-02-22 11:06:17,614 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-02-22 11:06:17,615 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-02-22 11:06:17,616 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-02-22 11:06:17,616 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-02-22 11:06:17,617 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-02-22 11:06:17,617 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-02-22 11:06:17,618 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-02-22 11:06:17,619 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-02-22 11:06:17,620 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-02-22 11:06:17,620 : INFO : EPOCH - 5 : training on 2083112 raw words (15152 effective words) took 17.7s, 855 effective words/s\n",
      "2021-02-22 11:06:17,621 : INFO : training on a 10415560 raw words (75620 effective words) took 89.4s, 846 effective words/s\n",
      "2021-02-22 11:06:17,622 : INFO : saving Word2Vec object under ./noteevents_clean_fasttext_win10_min5.dat, separately None\n",
      "2021-02-22 11:06:17,623 : INFO : not storing attribute vectors_norm\n",
      "2021-02-22 11:06:17,624 : INFO : not storing attribute cum_table\n",
      "2021-02-22 11:06:17,641 : INFO : saved ./noteevents_clean_fasttext_win10_min5.dat\n",
      "2021-02-22 11:06:17,642 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'attention' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-6c734426aafe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attention\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'attention' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# TRAIN AND SAVE W2V\n",
    "w2v_model = Word2Vec(sentences, size=size, window=window, min_count=min_count, workers=workers)\n",
    "w2v_model.save(filename.replace('.txt','_w2v_win10_min5.dat').replace('_tkns',''))\n",
    "\n",
    "# test\n",
    "w2v_model.wv.most_similar(\"attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2021-02-22 11:04:46,937 : INFO : collected 2019963 word types from a corpus of 2083112 raw words and 2083112 sentences\n",
    "2021-02-22 11:04:46,938 : INFO : Loading a fresh vocabulary\n",
    "2021-02-22 11:04:47,731 : INFO : effective_min_count=5 retains 1176 unique words (0% of original 2019963, drops 2018787)\n",
    "2021-02-22 11:04:47,732 : INFO : effective_min_count=5 leaves 23371 word corpus (1% of original 2083112, drops 2059741)\n",
    "2021-02-22 11:04:47,736 : INFO : deleting the raw counts dictionary of 2019963 items\n",
    "2021-02-22 11:04:47,993 : INFO : sample=0.001 downsamples 38 most-common words\n",
    "2021-02-22 11:04:47,994 : INFO : downsampling leaves estimated 15132 word corpus (64.7% of prior 23371)\n",
    "2021-02-22 11:04:47,997 : INFO : estimated required memory for 1176 words and 300 dimensions: 3410400 bytes\n",
    "2021-02-22 11:04:47,998 : INFO : resetting layer weights\n",
    "2021-02-22 11:04:48,240 : INFO : training model with 16 workers on 1176 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
    "2021-02-22 11:05:06,369 : INFO : EPOCH - 1 : training on 2083112 raw words (15121 effective words) took 18.1s, 834 effective words/s\n",
    "2021-02-22 11:05:24,363 : INFO : EPOCH - 2 : training on 2083112 raw words (15131 effective words) took 18.0s, 841 effective words/s\n",
    "2021-02-22 11:05:42,383 : INFO : EPOCH - 3 : training on 2083112 raw words (15072 effective words) took 18.0s, 837 effective words/s\n",
    "2021-02-22 11:05:59,885 : INFO : EPOCH - 4 : training on 2083112 raw words (15144 effective words) took 17.5s, 866 effective words/s\n",
    "2021-02-22 11:06:17,620 : INFO : EPOCH - 5 : training on 2083112 raw words (15152 effective words) took 17.7s, 855 effective words/s\n",
    "2021-02-22 11:06:17,621 : INFO : training on a 10415560 raw words (75620 effective words) took 89.4s, 846 effective words/s\n",
    "2021-02-22 11:06:17,622 : INFO : saving Word2Vec object under ./noteevents_clean_fasttext_win10_min5.dat, separately None\n",
    "2021-02-22 11:06:17,623 : INFO : not storing attribute vectors_norm\n",
    "2021-02-22 11:06:17,624 : INFO : not storing attribute cum_table\n",
    "2021-02-22 11:06:17,641 : INFO : saved ./noteevents_clean_fasttext_win10_min5.dat\n",
    "2021-02-22 11:06:17,642 : INFO : precomputing L2-norms of word weight vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'pain' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-fd4b3980c627>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pain\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'pain' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# test\n",
    "w2v_model.wv.most_similar(\"pain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eLBY60fHDTJ"
   },
   "source": [
    "## TRAIN TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iuImITKXFtra",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN AND SAVE TFIDF (USING SKLEARN - DOESN'T SEEM TO WORK ON BIG DATASETS)\n",
    "tfidf = TfidfVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram_range, sublinear_tf=True, use_idf=True, \n",
    "                        max_features=max_features, preprocessor=' '.join)\n",
    "trained_tfidf = tfidf.fit(sentences)\n",
    "with open(filename.replace('.txt','_tfidf.pickle'), 'wb') as fin:\n",
    "          pickle.dump(trained_tfidf, fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPuLq10MFtrc"
   },
   "outputs": [],
   "source": [
    "# TRAIN AND SAVE TFIDF (UING GENSIM)\n",
    "\n",
    "# Create the Dictionary and Corpus\n",
    "mydict = corpora.Dictionary(sentences) # create dctionary\n",
    "mydict.save(filename.replace('.txt','_tfidfdict.dict'))  # save dict to disk\n",
    "corpus = [mydict.doc2bow(line) for line in sentences] # create corpus\n",
    "corpora.MmCorpus.serialize(filename.replace('.txt','_tfidfcorpus.mm'), corpus)  # save corpus to disk\n",
    "\n",
    "tfidf_model = TfidfModel(corpus)  # fit model\n",
    "tfidf_model.save(filename.replace('.txt','_tfidf.model')) # save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "train_w2v_and_tfidf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
